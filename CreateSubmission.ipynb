{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2209670-d090-42da-a849-fa6ead5c6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "# from torchsummary import summary\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "from PIL import Image\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc29453-7e01-461a-8b21-fe478194f249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f352c6-f033-4dd5-93e9-b80034c32df4",
   "metadata": {
    "tags": []
   },
   "source": [
    " # Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640420a8-a3d3-4e41-bcd7-ae712b17f522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>cultivar</th>\n",
       "      <th>index_cultivar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-16__12-24-20-930.png</td>\n",
       "      <td>PI_257599</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-02__16-48-57-866.png</td>\n",
       "      <td>PI_154987</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-12__13-18-07-707.png</td>\n",
       "      <td>PI_92270</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-06-22__13-18-06-841.png</td>\n",
       "      <td>PI_152651</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-06-26__12-56-48-642.png</td>\n",
       "      <td>PI_176766</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image   cultivar  index_cultivar\n",
       "0  2017-06-16__12-24-20-930.png  PI_257599              73\n",
       "1  2017-06-02__16-48-57-866.png  PI_154987              29\n",
       "2  2017-06-12__13-18-07-707.png   PI_92270              99\n",
       "3  2017-06-22__13-18-06-841.png  PI_152651               6\n",
       "4  2017-06-26__12-56-48-642.png  PI_176766              50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'data/'\n",
    "TRAIN_PATH = PATH + 'train_images/'\n",
    "train_data_path = PATH + 'train_cultivar_mapping.csv'\n",
    "train_table = pd.read_csv(train_data_path)\n",
    "\n",
    "train_table = train_table.dropna()\n",
    "train_table['index_cultivar'] = train_table['cultivar'].astype('category')\n",
    "train_table['index_cultivar'] = train_table['index_cultivar'].cat.codes\n",
    "\n",
    "with open('mapping.pkl', 'rb') as f:\n",
    "    index_to_cultivar = pickle.load(f)\n",
    "\n",
    "tmp = {}\n",
    "for cultivar,index in zip(train_table['cultivar'],train_table['index_cultivar']):\n",
    "    if index in tmp.keys():\n",
    "        assert tmp[index] == cultivar\n",
    "    else:\n",
    "        tmp[index] = cultivar\n",
    "        \n",
    "assert tmp == index_to_cultivar\n",
    "train_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0f824-2ddf-4d6b-8551-23de8753af5c",
   "metadata": {
    "tags": []
   },
   "source": [
    " # Creating Vanilla Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7b88ee-171c-4d9b-bd9a-e69fece7aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "PATH = 'data/'\n",
    "TEST_PATH = PATH + 'test/'\n",
    "\n",
    "def apply_clahe(image):\n",
    "    image = np.asarray(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    image = cv2.split(image)\n",
    "    image = list(image)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    image[2] = clahe.apply(image[2])\n",
    "    image = cv2.merge(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "class SorghumDatasetTest(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.labels = df.values\n",
    "        self.image_path = TEST_PATH + df.values\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "             transforms.Resize(size=(512, 512)),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.labels[idx][0]\n",
    "        image_path = self.image_path[idx][0]\n",
    "        image = Image.open(image_path)\n",
    "        image = apply_clahe(image)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return {'image':image, 'filename': image_name}\n",
    "    \n",
    "\n",
    "def create_submission_file(model):\n",
    "    directory = 'data/test'\n",
    "    loader = SorghumDatasetTest(pd.DataFrame(os.listdir(directory)))\n",
    "    loader = torch.utils.data.DataLoader(loader,\n",
    "                                         batch_size=16,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=8)\n",
    "    submission = {\"filename\":[],\n",
    "                  \"cultivar\":[]}\n",
    "    #IMPORTANT######\n",
    "    model.eval()\n",
    "    ################\n",
    "    for i, data in tqdm(enumerate(loader, 0)):\n",
    "            # get the inputs\n",
    "            image, filenames = data['image'],data['filename']\n",
    "            with torch.no_grad():\n",
    "                prediction_index = model(image.cuda()).argmax(1).cpu().detach()\n",
    "            for i in range(len(filenames)):\n",
    "                prediction = index_to_cultivar[prediction_index[i].item()]\n",
    "                submission[\"filename\"].append(filenames[i])\n",
    "                submission[\"cultivar\"].append(prediction)\n",
    "    model = model.cpu()\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826a00fb-9355-4593-9157-91662e424edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = 'efficientNetb4-FullSettings_pretrained'\n",
    "# PATH = f'models/{modelname}'\n",
    "# loading_model = torch.load(PATH)\n",
    "# submision = create_submission_file(loading_model.cuda())\n",
    "# sub = pd.DataFrame.from_dict(submision)\n",
    "# print(sub)\n",
    "# sub.to_csv(f'Submissions/{modelname}_vanilla.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d72d9-c647-4882-b55a-0fcd6f9acc01",
   "metadata": {
    "tags": []
   },
   "source": [
    " # Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b53c7-ce8d-418f-9afd-78b77fd59ed4",
   "metadata": {},
   "source": [
    "In this section we attempt to make the testing set more similar to the training set. To do this we do the following.\n",
    "For a given testing image, we apply 15 sampled transformations (from the same distribution as in the training set). We then sum the resulting probability vectors from the evalution of these new transformed images. The prediction is the argmax of the sum of this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e563b95-acf8-4122-a5e2-9c1bf2278518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "PATH = 'data/'\n",
    "TEST_PATH = PATH + 'test/'\n",
    "#random choice class from\n",
    "#https://stackoverflow.com/questions/65447992/pytorch-how-to-apply-the-same-random-transformation-to-multiple-image\n",
    "\n",
    "import random\n",
    "\n",
    "class RandomChoice(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t = random.choice(self.transforms)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.t(img)\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "def apply_clahe(image):\n",
    "    image = np.asarray(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    image = cv2.split(image)\n",
    "    image = list(image)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    image[2] = clahe.apply(image[2])\n",
    "    image = cv2.merge(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "class SorghumDatasetTestEnsemble(Dataset):\n",
    "    def __init__(self, df, transform=None,number_of_transforms= 20):\n",
    "        self.labels = df.values\n",
    "        self.image_path = TEST_PATH + df.values\n",
    "        trans = [\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=(0, 180)),\n",
    "                transforms.RandomPerspective()\n",
    "                \n",
    "            ]),\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomGrayscale(p=0.1),\n",
    "                transforms.ColorJitter(brightness=.3, hue=.4),\n",
    "                transforms.GaussianBlur(kernel_size=5),\n",
    "                transforms.RandomInvert(p=0.1)\n",
    "            ]),\n",
    "            \n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomResizedCrop(size=(512,512),scale=(0.1,1.00)),\n",
    "                transforms.Resize(size=(512, 512)),\n",
    "            ]),\n",
    "            \n",
    "            \n",
    "            \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            # transforms.RandomErasing(inplace=True),\n",
    "        ]\n",
    "        self.transform = transforms.Compose(trans)\n",
    "        self.n_trans = number_of_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.labels[idx][0]\n",
    "        image_path = self.image_path[idx][0]\n",
    "        image = Image.open(image_path)\n",
    "        image = apply_clahe(image)\n",
    "        images = []\n",
    "        for _ in range(self.n_trans): # number needs to be changed wrt how many images we can fit in the gpu\n",
    "            image_transformed = self.transform(image)\n",
    "            images.append(image_transformed.unsqueeze(0))\n",
    "\n",
    "        \n",
    "        return {'image':torch.cat(images,0), 'filename': image_name}\n",
    "    \n",
    "\n",
    "def create_submission_file_ensembling(model):\n",
    "    OUTPUT_FILE = open('output.txt', 'w')\n",
    "    directory = 'data/test'\n",
    "    loader = SorghumDatasetTestEnsemble(pd.DataFrame(os.listdir(directory)))\n",
    "    n_trans = loader.n_trans\n",
    "    loader = torch.utils.data.DataLoader(loader,batch_size=1,shuffle=False,num_workers=8)\n",
    "    submission = {\"filename\":[],\"cultivar\":[]}\n",
    "    #IMPORTANT######\n",
    "    model.eval()\n",
    "    ################\n",
    "    n = len(loader)\n",
    "    BATCH_SIZE = 6\n",
    "    for i, data in tqdm(enumerate(loader, 0),file=OUTPUT_FILE):\n",
    "            OUTPUT_FILE.write(f\"Iteration {i}/{n}\\n\")\n",
    "            OUTPUT_FILE.flush()\n",
    "            images, filenames = data['image'],data['filename']\n",
    "            model = model.cuda()\n",
    "            #do it 2 times\n",
    "            images_squeezed = images.squeeze(0)\n",
    "            prediction_vectors = []\n",
    "            for bi in range(0,n_trans//BATCH_SIZE+1):\n",
    "            \n",
    "                images_batch = images_squeezed[bi*BATCH_SIZE:\n",
    "                                               min((bi+1)*BATCH_SIZE,images_squeezed.shape[0])]\n",
    "\n",
    "                prediction_indexes = model(images_batch.cuda())\n",
    "                images_batch.cpu()\n",
    "                prediction_indexes = prediction_indexes.cpu().detach()\n",
    "                prediction_vectors.append(prediction_indexes)\n",
    "            prediction_indexes = torch.concat(prediction_vectors)\n",
    "\n",
    "            ensemble_pred = prediction_indexes.sum(0).argmax(0)\n",
    "            prediction = index_to_cultivar[ensemble_pred.item()]\n",
    "            submission[\"filename\"].append(filenames[0])\n",
    "            submission[\"cultivar\"].append(prediction)\n",
    "    OUTPUT_FILE.close()\n",
    "    return submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba8e82-f2e7-4d7c-9aba-b2bc56fb562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23932271978e4d04b5348a14b9a6257e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelname = 'efficientNetb4-FullSettings_pretrained'\n",
    "PATH = f'models/{modelname}'\n",
    "loading_model = torch.load(PATH)\n",
    "submision = create_submission_file_ensembling(loading_model.cuda())\n",
    "sub = pd.DataFrame.from_dict(submision)\n",
    "print(sub)\n",
    "sub.to_csv(f'Submissions/{modelname}_ensemble_averaging_prob_easiest_version.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29c2f7-5067-4780-8e02-cc67340bc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "PATH = 'data/'\n",
    "TEST_PATH = PATH + 'test/'\n",
    "#random choice class from\n",
    "#https://stackoverflow.com/questions/65447992/pytorch-how-to-apply-the-same-random-transformation-to-multiple-image\n",
    "\n",
    "import random\n",
    "\n",
    "class RandomChoice(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t = random.choice(self.transforms)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.t(img)\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "def apply_clahe(image):\n",
    "    image = np.asarray(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    image = cv2.split(image)\n",
    "    image = list(image)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    image[2] = clahe.apply(image[2])\n",
    "    image = cv2.merge(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "class SorghumDatasetTestEnsemble(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.labels = df.values\n",
    "        self.image_path = TEST_PATH + df.values\n",
    "        trans = [\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=(0, 180)),\n",
    "                transforms.RandomPerspective()\n",
    "                \n",
    "            ]),\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomGrayscale(p=0.1),\n",
    "                transforms.ColorJitter(brightness=.1, hue=.4),\n",
    "                transforms.GaussianBlur(kernel_size=5),\n",
    "                transforms.RandomInvert(p=0.1)\n",
    "            ]),\n",
    "            \n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomResizedCrop(size=(512,512),scale=(0.1,1.00)),\n",
    "                transforms.Resize(size=(512, 512)),\n",
    "            ]),\n",
    "            \n",
    "            \n",
    "            \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            transforms.RandomErasing(inplace=True),\n",
    "        ]\n",
    "        self.transform = transforms.Compose(trans)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.labels[idx][0]\n",
    "        image_path = self.image_path[idx][0]\n",
    "        image = Image.open(image_path)\n",
    "        image = apply_clahe(image)\n",
    "        images = []\n",
    "        for _ in range(16):\n",
    "            #TODO: COMPARE ADDING PROBS AND ADDING MAX\n",
    "            image_transformed = self.transform(image)\n",
    "            images.append(image_transformed.unsqueeze(0))\n",
    "\n",
    "        \n",
    "        return {'image':torch.cat(images,0), 'filename': image_name}\n",
    "    \n",
    "\n",
    "def create_submission_file_ensembling(model):\n",
    "    OUTPUT_FILE = open('output.txt', 'w')\n",
    "    directory = 'data/test'\n",
    "    loader = SorghumDatasetTestEnsemble(pd.DataFrame(os.listdir(directory)))\n",
    "    loader = torch.utils.data.DataLoader(loader,batch_size=1,shuffle=False,num_workers=8)\n",
    "    submission = {\"filename\":[],\"cultivar\":[]}\n",
    "    #IMPORTANT######\n",
    "    model.eval()\n",
    "    ################\n",
    "    n = len(loader)\n",
    "    for i, data in tqdm(enumerate(loader, 0),file=OUTPUT_FILE):\n",
    "            OUTPUT_FILE.write(f\"Iteration {i}/{n}\\n\")\n",
    "            OUTPUT_FILE.flush()\n",
    "            images, filenames = data['image'],data['filename']\n",
    "            model = model.cuda()\n",
    "            cuda_images = images.squeeze(0).cuda()\n",
    "            prediction_indexes = model(cuda_images)\n",
    "            prediction_indexes = prediction_indexes.cpu().detach()\n",
    "            images = cuda_images.detach()\n",
    "            second = model(cuda_images)\n",
    "            print(prediction_indexes.shape)\n",
    "            print(second.shape)\n",
    "            prediction_indexes = torch.concatenate(prediction_indexes,\n",
    "                                                   second.cpu().detach())\n",
    "            print(prediction_indexes.shape)\n",
    "            break\n",
    "            \n",
    "            # ensemble_pred = prediction_indexes.argmax(1)\n",
    "            \n",
    "            ensemble_pred = torch.mode(prediction_indexes.argmax(1),0).values\n",
    "            prediction = index_to_cultivar[ensemble_pred.item()]\n",
    "            submission[\"filename\"].append(filenames[0])\n",
    "            submission[\"cultivar\"].append(prediction)\n",
    "    OUTPUT_FILE.close()\n",
    "    return submission\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039c325-8b4b-415c-b1d7-e25d0b94f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'efficientNetb0-FullDataAugmentation-512-retrainedv5_clahe'\n",
    "PATH = f'models/{modelname}'\n",
    "loading_model = torch.load(PATH)\n",
    "submision = create_submission_file_ensembling(loading_model.cuda())\n",
    "sub = pd.DataFrame.from_dict(submision)\n",
    "print(sub)\n",
    "sub.to_csv(f'Submissions/{modelname}_ensemble_maxes.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5fa0d-31d6-482d-99c5-340d11845e61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " # Using features and classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43c39b-dab3-4ffc-87a2-3188aecf909b",
   "metadata": {},
   "source": [
    "Not used. Bad results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc01d9-44bd-4f5d-9b5e-f34021010153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_submission_file_classifier(clf):\n",
    "#     directory = 'data/test'\n",
    "#     loader = SorghumDatasetTest(pd.DataFrame(os.listdir(directory)))\n",
    "#     loader = torch.utils.data.DataLoader(loader,batch_size=16,shuffle=False,num_workers=2)\n",
    "#     submission = {\"filename\":[],\"cultivar\":[]}\n",
    "    \n",
    "#     for i, data in tqdm(enumerate(loader, 0)):\n",
    "#             # get the inputs\n",
    "#             image, filenames = data['image'],data['filename']\n",
    "            \n",
    "#             output = model.features(image.cuda())\n",
    "#             output = model.avgpool(output).cpu().detach().numpy()\n",
    "#             prediction_index = clf.predict(output)\n",
    "#             for i in range(len(filenames)):\n",
    "#                 prediction = index_to_cultivar[prediction_index[i].item()]\n",
    "#                 submission[\"filename\"].append(filenames[i])\n",
    "#                 submission[\"cultivar\"].append(prediction)\n",
    "#     return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabf94a-ad05-4292-8bad-648c261e126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('embeddings/efficientNetb0-FullDataAugmentation-512-retrainedv3_clahe_training_notrans.npy', 'rb') as f:\n",
    "#     features = np.load(f,allow_pickle=True)\n",
    "#     labels = np.load(f,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba569d-c257-4275-a3ab-75160fd397c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.decomposition import PCA\n",
    "# clf = SVC(gamma='auto')\n",
    "# pca = PCA(n_components=10)\n",
    "# print(\"Transforming\")\n",
    "# features_red = pca.fit_transform(features)\n",
    "# print(\"Fitting\")\n",
    "# clf.fit(features_red,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e649bd-0a1a-4818-b227-38ac44ff772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.score(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a4abf-2967-44fe-9504-3915d0055f13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " # Trash code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4149797-e1f8-4011-a2b2-5e643e8411ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SorghumDataset(Dataset):\n",
    "#     def __init__(self, df,train=True):\n",
    "#         self.image_path = TRAIN_PATH + df['image'].values\n",
    "#         self.labels = df[\"index_cultivar\"].values\n",
    "#         transformers = []\n",
    "#         if train:\n",
    "#             transformers += [\n",
    "#              # transforms.Grayscale(3),\n",
    "#              # transforms.RandomEqualize(p=1),\n",
    "#              transforms.RandomHorizontalFlip(p=0.5),\n",
    "#              transforms.RandomVerticalFlip(p=0.5)]\n",
    "#         transformers += [\n",
    "#              transforms.Resize(size=(512, 512)),\n",
    "#              transforms.ToTensor(),\n",
    "#              #mean computed from all coordinates\n",
    "#              # transforms.Normalize(mean=[0.3445],\n",
    "#              #                     std=[0.0381])\n",
    "#             #mean obtained from the pytorch website for transfer learning\n",
    "#             # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#             #                      std=[0.229, 0.224, 0.225])\n",
    "#             # mean computed for each coordinate\n",
    "#                 # transforms.Normalize(mean=[0.35835076, 0.38230668, 0.29287688],\n",
    "#                 #                  std=[1.81370149e-04, 1.77726425e-04, 8.24954614e-05])\n",
    "#             ]\n",
    "#         self.transform = transforms.Compose(transformers)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "#         image_path = self.image_path[idx]\n",
    "        \n",
    "#         image = Image.open(image_path)\n",
    "#         image = self.transform(image)\n",
    "        \n",
    "#         return {'image':image, 'target': label}\n",
    "# idx = np.arange(len(train_table))\n",
    "# np.random.shuffle(idx)\n",
    "# cut = int(len(train_table)*0.8)\n",
    "# idx_train = idx[0:cut]\n",
    "# idx_val = idx[cut:]\n",
    "# print(f\"Train size {len(idx_train)}, test size {len(idx_val)} \")\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def accuracy(net, test_loader, cuda=True):\n",
    "#     net.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     # loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in test_loader:\n",
    "#             images, labels = data['image'],data['target']\n",
    "#             if cuda:\n",
    "#                 images = images.type(torch.cuda.FloatTensor)\n",
    "#                 labels = labels.type(torch.cuda.LongTensor)\n",
    "#             outputs = net(images)\n",
    "#             # loss+= criterion(outputs, labels).item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#           # if total > 100:\n",
    "#             # break\n",
    "#     net.train()\n",
    "#     acc = correct / total\n",
    "#     print(f'Accuracy of the network on the test images:{(100 *acc )}')\n",
    "#     #return (100.0 * correct / total, loss/total)\n",
    "#     return correct/total\n",
    "\n",
    "# def train(net, optimizer, train_loader, test_loader, criterion,  n_epoch = 5,\n",
    "#           train_acc_period = 100,\n",
    "#           test_acc_period = 5,\n",
    "#           cuda=True):\n",
    "#     train_acc = []\n",
    "#     test_acc = []\n",
    "#     total = 0\n",
    "#     for epoch in tqdm(range(n_epoch)):  # loop over the dataset multiple times\n",
    "#         running_loss = 0.0\n",
    "#         running_acc = 0.0\n",
    "#         net.train()\n",
    "#         for i, data in enumerate(train_loader, 0):\n",
    "#             # get the inputs\n",
    "#             inputs, labels = data['image'],data['target']\n",
    "#             if cuda:\n",
    "#                 inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "#                 labels = labels.type(torch.cuda.LongTensor)\n",
    "#             # print(inputs.shape)\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # forward + backward + optimize\n",
    "#             outputs = net(inputs)\n",
    "\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total += labels.size(0)\n",
    "#             # print statistics\n",
    "#             running_loss = 0.33*loss.item()/labels.size(0) + 0.66*running_loss\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             correct = (predicted == labels).sum().item()/labels.size(0)\n",
    "#             running_acc = 0.3*correct + 0.66*running_acc\n",
    "#             if i % train_acc_period == train_acc_period-1:\n",
    "#                 train_acc.append(running_acc)\n",
    "#                 print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss))\n",
    "#                 print('[%d, %5d] acc: %.3f' %(epoch + 1, i + 1, running_acc))\n",
    "#                 running_loss = 0.0\n",
    "#                 total = 0\n",
    "#                 # break\n",
    "                \n",
    "            \n",
    "#         cur_acc = accuracy(net, test_loader, cuda=cuda)\n",
    "#         test_acc.append(cur_acc)\n",
    "#         print('[%d] acc: %.3f' %(epoch + 1, cur_acc))\n",
    "\n",
    "#     print('Finished Training')\n",
    "#     return train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767ebd2-6c35-4245-975a-69b1f39c2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_transfer = models.efficientnet_b2()\n",
    "# net_transfer.classifier\n",
    "# def net_classifier(num_classes):\n",
    "#     classifier = nn.Sequential(\n",
    "#         nn.Dropout(p=0.2),\n",
    "#         nn.Linear(1792,num_classes)\n",
    "#         # nn.ReLU(inplace=True),\n",
    "#         # nn.Dropout(),\n",
    "#         # nn.Linear(4096,2048),\n",
    "#         # nn.ReLU(inplace=True),\n",
    "#         # nn.Linear(2048, num_classes)\n",
    "#     )\n",
    "#     return classifier\n",
    "\n",
    "# def net(num_classes, is_pretrained=False, **kwargs):\n",
    "#     model = models.efficientnet_b4(pretrained=is_pretrained) # CHANGE MODEL HERE\n",
    "#     classifier = net_classifier(num_classes)\n",
    "#     model.classifier = classifier\n",
    "#     return model\n",
    "# training_data = SorghumDataset(train_table.iloc[idx_train])\n",
    "# val_data = SorghumDataset(train_table.iloc[idx_val],train=False)\n",
    "# trainloader = torch.utils.data.DataLoader(training_data,batch_size=32,shuffle=True,num_workers=8)\n",
    "# valloader = torch.utils.data.DataLoader(val_data,batch_size=32,shuffle=True,num_workers=8)\n",
    "# network = net(num_classes=100, pretrained=False)\n",
    "# use_cuda = True\n",
    "# if use_cuda and torch.cuda.is_available():\n",
    "#     print(\"using cuda\")\n",
    "#     network = network.cuda()\n",
    "# learning_rate = 1e-3\n",
    "\n",
    "# #for pretrained\n",
    "# # optimizer = torch.optim.Adam([\n",
    "# #                 {'params': network.features.parameters(), 'lr': learning_rate/10},\n",
    "# #                 {'params': network.classifier.parameters()}],\n",
    "# #             lr=learning_rate)\n",
    "\n",
    "# #for not pretrained\n",
    "# optimizer = torch.optim.Adam(network.parameters(),lr=learning_rate)\n",
    "\n",
    "# train_acc_netPretrained, test_acc_netPretrained = train(network, optimizer, trainloader, valloader, criterion,  n_epoch = 20,\n",
    "#       train_acc_period = 10,\n",
    "#       test_acc_period = 100)\n",
    "# modelname = 'modelefficientnetb4_dataAug_fullsize_notnormalized_notPretrained'\n",
    "# # modelname = 'modelefficientnetb4'\n",
    "# PATH = f'models/{modelname}'\n",
    "# torch.save(network, PATH)\n",
    "\n",
    "\n",
    "# with open(f'plots/{modelname}.npy', 'wb') as f:\n",
    "#     np.save(f, train_acc_netPretrained)\n",
    "#     np.save(f, test_acc_netPretrained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
